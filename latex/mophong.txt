Thiết lập mô phỏng
Đội hình gồm 3 AUV với một AUV leader và hai AUV follower. Leader (AUV1) di chuyển theo một lộ trình tuần hoàn qua các điểm định trước (waypoints) trong mặt phẳng ngang. Cụ thể, lộ trình bao gồm tuần tự các điểm: (0, 0) → (20, -13) → (10, -23) → (-10, -8) rồi quay về (0, 0); chu trình này lặp lại 2 vòng (tổng cộng 9 điểm, kể cả điểm lặp lại). Tốc độ di chuyển của leader được giả định không đổi (≈2 m/s). Tại mỗi điểm waypoint, leader đổi hướng đột ngột (không giảm tốc), tạo thành các góc chuyển hướng đáng kể (~90° hoặc lớn hơn).
Hai AUV follower (AUV2 và AUV3) có nhiệm vụ bám theo leader và duy trì vị trí tương đối mong muốn trong đội hình. Vị trí mong muốn của mỗi follower được xác định bởi vector khoảng cách Δ so với leader. Ở đây: Δ₁₂ = (-5, 2) m (follower 1 ở phía sau bên trái leader) và Δ₁₃ = (-5, -2) m (follower 2 ở phía sau bên phải) trong hệ trục gắn với leader. Nói cách khác, nếu coi hướng chuyển động trước mặt của leader là trục x địa phương, thì follower 1 luôn duy trì khoảng cách 5 m phía sau và 2 m chếch về bên trái leader; follower 2 duy trì 5 m phía sau và 2 m về bên phải. Vị trí đích tức thời của mỗi follower tại thời điểm t được tính bằng vị trí leader cộng vector Δ tương ứng xoay theo hướng của leader.
Để bám theo leader, mỗi follower sử dụng một thuật toán điều khiển học tăng cường đa tác tử (MADDPG) với mạng nơ-ron RBF trong thành phần tác nhân (actor). Mạng RBF giúp xấp xỉ hàm chính sách liên tục một cách mượt mà: các neuron RBF được kích hoạt theo hàm Gaussian của trạng thái đầu vào. Tại mỗi bước thời gian, agent của follower quan sát trạng thái cục bộ (bao gồm thông tin tương đối với leader như khoảng cách, góc phương vị, vận tốc tương đối, v.v.) và xuất ra tín hiệu điều khiển vận tốc cho follower đó. Nhờ có cấu trúc MADDPG, các follower học phối hợp với nhau để duy trì đội hình.
Trong mô phỏng này, mình giả lập chính sách MADDPG+RBF bằng bộ điều khiển vận tốc tương đối đơn giản: mỗi follower sẽ chạy với vận tốc gần bằng vận tốc của leader, đồng thời có thành phần hiệu chỉnh tỷ lệ với sai số đội hình hiện tại. Sai số đội hình của follower j được định nghĩa là khoảng cách giữa vị trí thực tế của follower và vị trí mong muốn (tương ứng với vector Δ₁ⱼ), ký hiệu $e_j(t) = | \eta_j(t) - \eta_{j,\text{des}}(t)|$. Bộ điều khiển của follower điều chỉnh vận tốc theo hướng giảm sai số này: vj(t)=vleader(t)+Kp(ηj,des(t)−ηj(t)),\mathbf{v}_{j}(t) = \mathbf{v}_{\text{leader}}(t) + K_p \big(\eta_{j,\text{des}}(t) - \eta_j(t)\big),vj​(t)=vleader​(t)+Kp​(ηj,des​(t)−ηj​(t)), trong đó $K_p$ là hệ số lợi ích (đặt nhỏ để tránh dao động). Nhờ đó, khi follower lệch khỏi vị trí chuẩn, nó sẽ tăng tốc hoặc giảm tốc (và đổi hướng) tương ứng để dần tiệm cận lại khoảng cách mong muốn với leader.
thuật toán MADDPG (Multi-Agent Deep Deterministic Policy Gradient) đã được áp dụng để điều khiển đội hình nhiều robot/AUV do khả năng học chính sách liên tục trong môi trường đa tác nhân. MADDPG sử dụng cách tiếp cận huấn luyện tập trung – chính sách phân tán: trong giai đoạn huấn luyện, các tác nhân chia sẻ thông tin để tối ưu chính sách, nhưng khi triển khai, mỗi tác nhân điều khiển độc lập dựa trên quan sát cục bộ. Để nâng cao năng lực xấp xỉ hàm của mạng chính sách (actor), nghiên cứu này tích hợp mạng nơ-ron RBF (Radial Basis Function) vào kiến trúc của actor. Mạng RBF với các hàm kích hoạt Gaussian có thể giúp xấp xỉ các hàm phi tuyến một cách trơn tru, cải thiện độ ổn định và tốc độ hội tụ của thuật toán học. Thuật toán MADDPG+RBF được áp dụng để điều khiển đội hình 3 AUV, trong đó AUV dẫn đầu di chuyển theo quỹ đạo tham chiếu cho trước, hai AUV theo sau cần học cách điều chỉnh vận tốc của mình để duy trì khoảng cách không đổi với AUV dẫn đầu.
Mô hình động học AUV: Mỗi AUV được mô hình hóa chuyển động trên mặt phẳng 2D với trạng thái bao gồm tọa độ vị trí $η_i(t) = [x_i(t),, y_i(t)]^T$ và vận tốc điều khiển tương ứng $v_{x,i}(t), v_{y,i}(t)$. Phương trình động học dạng rút gọn có thể viết: $\dot{x}i(t) = v{x,i}(t),; \dot{y}i(t) = v{y,i}(t)$, hay dạng vector $\dot{η}i(t) = v_i(t)$ tương ứng với vận tốc điều khiển $v_i(t) = [v{x,i}, v_{y,i}]^T$. Giả thiết các AUV di chuyển ở vận tốc thấp và khu vực hoạt động hạn chế, ta có thể bỏ qua ảnh hưởng của quán tính hay động lực học phức tạp, tập trung vào điều khiển vị trí (điều khiển động học).
Quỹ đạo AUV dẫn đầu: AUV 1 (leader) di chuyển theo quỹ đạo hình sin trong mặt phẳng $XY$. Cụ thể, quỹ đạo tham chiếu được cho bởi:
    • $x_1(t) = v \cdot t$ – chuyển động thẳng đều theo trục $X$ với vận tốc $v$ (m/s),
    • $y_1(t) = A \cdot \sin(ω t)$ – dao động hình sin theo trục $Y$ với biên độ $A$ và tần số góc $ω$.
Như vậy, AUV dẫn đầu sẽ dao động quanh trục $X$ khi tiến về phía trước, tạo thành đường cong hình sin. Quỹ đạo này được lấy làm quỹ đạo mục tiêu để các AUV theo sau bám đuổi.
Đội hình mong muốn: Để duy trì cấu hình đội hình, mỗi AUV theo sau (AUV $j$, $j=2,3$) được yêu cầu giữ một vị trí tương đối cố định so với AUV 1. Gọi $Δ_{1j}$ là vectơ khoảng cách mong muốn từ AUV 1 đến AUV $j$. Khi đó, vị trí mong muốn của AUV $j$ tại thời điểm $t$ được xác định bởi:
    • $η_{des,j}(t) = η_1(t) + Δ_{1j}$.
Nói cách khác, $η_{des,j}$ chính là tọa độ mà AUV $j$ cần đạt được nếu bám đúng đội hình. Ví dụ, nếu chọn $Δ_{12} = (-5 \text{ m},; 2 \text{ m})$ và $Δ_{13} = (-5 \text{ m},; -2 \text{ m})$, điều này tương ứng với việc AUV 2 và 3 luôn nằm phía sau AUV 1 một khoảng 5 m, lần lượt lệch lên trên 2 m và lệch xuống dưới 2 m. Giá trị $Δ_{1j}$ có thể điều chỉnh tùy theo cấu hình đội hình mong muốn (hình mũi tên, hàng dọc, v.v.).
Sai số đội hình: Sai số bám đội hình của AUV $j$ được định nghĩa là khoảng cách giữa vị trí thực tế của nó $η_j(t)$ và vị trí mong muốn $η_{des,j}(t)$. Ta ký hiệu $e_j(t)$ là độ lớn của sai lệch này:
    • $e_j(t) = |,η_j(t) - η_{des,j}(t),|$, tức là độ dài của véc-tơ chênh lệch tọa độ.
Khai triển tọa độ, ta có $e_j(t) = \sqrt{\left(x_j(t) - x_{des,j}(t)\right)^2 + \left(y_j(t) - y_{des,j}(t)\right)^2}$. Mục tiêu điều khiển là làm cho $e_j(t) \to 0$ (hoặc nhỏ nhất có thể) với mọi $t$, tức AUV $j$ di chuyển trùng với vị trí mong muốn so với leader.
Thuật toán điều khiển (MADDPG + RBF): Để các AUV theo sau có thể bám theo leader một cách hiệu quả trong điều kiện bất định, nghiên cứu sử dụng thuật toán học tăng cường sâu MADDPG kết hợp mạng RBF. Mỗi AUV theo sau được xem là một tác nhân (agent) trong môi trường đa tác nhân hợp tác. Các agent này được huấn luyện để tối đa hóa hàm thưởng chung, qua đó học được chính sách điều khiển giúp duy trì đội hình.
    • Trạng thái đầu vào: Tại mỗi bước thời gian, agent $j$ (AUV theo sau) quan sát trạng thái bao gồm thông tin về chính nó và tương quan với leader. Cụ thể, vector trạng thái có thể bao gồm: tọa độ và vận tốc của chính AUV $j$, cũng như độ lệch vị trí so với AUV 1. Một ví dụ trạng thái đầu vào như sau: $s_j = [,x_j,; y_j,; \dot{x}_j,; \dot{y}_j,; x_1 - x_j,; y_1 - y_j,]$. Với cách mã hóa này, tác nhân nhận biết được mình đang ở đâu và tình trạng tương đối so với AUV dẫn đầu.
    • Chính sách điều khiển (Actor): Mạng neural của actor được thiết kế tích hợp lớp RBF để xấp xỉ hàm chính sách liên tục. Lớp RBF gồm $M$ nút ẩn với hàm kích hoạt dạng Gaussian: $\phi_k(s) = \exp!\big(-γ |,s - c_k,|^2\big)$, $k = 1,...,M$. Trong đó $c_k$ là vector tâm của nút RBF thứ $k$ trong không gian trạng thái, $γ$ là hệ số điều chỉnh độ rộng. Đầu ra của mạng actor chính là tín hiệu điều khiển (tương ứng với vận tốc đặt cho AUV trong mô phỏng này), được tính bằng kết hợp tuyến tính các hàm RBF: $a_j = W^T \Phi(s_j) + b$. Ở đây $\Phi(s_j) = [\phi_1(s_j),...,\phi_M(s_j)]^T$ là vector đầu ra của các nút RBF, $W$ là vector trọng số và $b$ là hệ số điều chỉnh (bias). Nhờ sử dụng các hàm nền RBF, mạng actor có khả năng xấp xỉ các hàm phi tuyến phức tạp một cách mượt mà hơn so với mạng MLP truyền thống, qua đó cải thiện chất lượng chính sách điều khiển.
    • Critic: MADDPG sử dụng mạng critic để ước lượng hàm giá trị hành động $Q$ cho toàn bộ hệ thống đa tác nhân, trong đó đầu vào bao gồm trạng thái và hành động của tất cả các agent. Mạng critic này giúp tính toán gradient cập nhật cho actor của từng agent (theo thuật toán DDPG mở rộng cho đa tác nhân). Trong phạm vi bài báo, chi tiết về critic tập trung vào cơ chế huấn luyện, còn trong quá trình thực thi (deployment), chỉ mạng actor của từng AUV được sử dụng để sinh hành động.
    • Hàm mục tiêu và phần thưởng: Mục tiêu của các agent là giảm thiểu sai số đội hình và tránh va chạm, đồng thời tiết kiệm năng lượng điều khiển. Do đó, hàm phần thưởng cho mỗi agent $j$ có thể thiết kế dựa trên các tiêu chí này. Ví dụ một hàm thưởng điển hình: $r_j = -α , e_j^2 - β ,\text{(collision penalty)} - γ , |a_j|^2$. Trong đó, $e_j$ là sai số đội hình hiện tại, $α > 0$ là hệ số phạt lỗi vị trí (thường được chọn lớn nhất để ưu tiên giảm sai số), thành phần phạt va chạm (collision penalty) kích hoạt khi khoảng cách giữa các AUV quá gần (để đảm bảo an toàn đội hình), và $γ |a_j|^2$ là mục tiêu giảm thiểu độ lớn hành động (phạt năng lượng điều khiển, giúp chuyển động mượt hơn). Thông qua việc tối ưu hàm thưởng này, tác nhân sẽ học được cách bám theo đội hình một cách hiệu quả và an toàn.
Tóm lại, thuật toán MADDPG+RBF cung cấp một khung học tập cho phép các AUV theo sau dần dần học được chiến lược điều khiển tối ưu để giữ vững đội hình. Mạng RBF trong actor đóng vai trò cải thiện khả năng xấp xỉ hàm chính sách, trong khi cấu trúc MADDPG đảm bảo tính phối hợp giữa các agent trong quá trình huấn luyện. Sau khi huấn luyện, mỗi AUV có thể tự hành điều chỉnh vận tốc dựa trên quan sát trạng thái, nhằm duy trì vị trí tương đối cố định so với AUV dẫn đầu.

Điều kiện ban đầu: 
tại thời điểm $t=0$, leader ở tọa độ (0, 0) và các follower được bố trí đúng vị trí đội hình (sai số ban đầu ≈ 0). Điều này giúp quá trình mô phỏng tập trung đánh giá sai số phát sinh do hành vi chuyển động (đặc biệt tại các góc rẽ) hơn là do lệch pha ban đầu.
Môi trường mô phỏng trạng thái được tái sử dụng theo plugin buoyancy, một plugin chuyên được sử dụng cho việc mô phỏng chuyển động dưới nước.
